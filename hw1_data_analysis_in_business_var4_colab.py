# -*- coding: utf-8 -*-
"""HW1_Data_Analysis_in_Business_var4_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10FoDRuwAuRMtWrTV1jUI6iz5tx2QYKNM

# Homework 1 — Data Analysis in Business (Variant 4)
"""

import sys, os, io, textwrap, json, math, pathlib, itertools, re, warnings, gc, random
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay

try:
    from google.colab import files
    uploaded = files.upload()
    fname = list(uploaded.keys())[0]
except Exception as e:
    fname = "/Users/anastasiaseraa/Downloads/HW1_var_4.csv"

df = pd.read_csv(fname, sep=";", encoding="utf-8")
print(df.shape)
df.head(3)

"""## Part 1 — Data loading & structure"""

print("Shape:", df.shape)
print("\nDtypes:")
print(df.dtypes)

print("\nDescribe (numeric):")
display(df.describe().T)

print("\nFirst 10 rows:")
display(df.head(10))

missing = df.isna().mean().sort_values(ascending=False).to_frame("share_missing").reset_index().rename(columns={"index":"column"})
display(missing.head(30))

"""## Part 2 — Exploratory Data Analysis (EDA)"""

priority = ["dlq_exist","ninety_in_a_year","sixty_in_a_year","thirty_in_a_year","ninety_vintage","sixty_vintage","thirty_vintage"]
primary_target = None
for c in priority:
    if c in df.columns:
        primary_target = c
        break

print("Primary target:", primary_target)
assert primary_target is not None, "No suitable target was found. Please check column names."

vc = df[primary_target].value_counts(dropna=False).sort_index()
print("\nTarget value counts:")
print(vc)

plt.figure()
vc.plot(kind="bar")
plt.xlabel(primary_target)
plt.ylabel("Count")
plt.title(f"Distribution of {primary_target}")
plt.tight_layout()
plt.savefig("target_distribution.png")
plt.show()

if "DTI" in df.columns and df["DTI"].dtype == "object":
    dti_num = (df["DTI"].astype(str)
               .str.replace(",", ".", regex=False)
               .str.replace(" ", "", regex=False)
               .str.replace("%", "", regex=False))
    df["DTI_num"] = pd.to_numeric(dti_num, errors="coerce")
    print("DTI converted to numeric")
else:
    print("No textual DTI to convert or already numeric.")

if "DTI_num" in df.columns:
    plt.figure()
    df["DTI_num"].dropna().plot(kind="hist", bins=40)
    plt.xlabel("DTI")
    plt.ylabel("Frequency")
    plt.title("DTI distribution")
    plt.tight_layout()
    plt.savefig("dti_distribution.png")
    plt.show()

if pd.api.types.is_numeric_dtype(df[primary_target]):
    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c not in ["ID","Номер варианта"]]
    corr_series = df[numeric_cols].corr(numeric_only=True)[primary_target].sort_values(ascending=False)
    corr_table = corr_series.to_frame("pearson_corr_with_target").reset_index().rename(columns={"index":"feature"})
    display(corr_table.head(30))
else:
    print("Target is non-numeric, skip Pearson correlation.")

cat_candidates = ["INCOME_BASE_TYPE","CREDIT_PURPOSE","SEX","EDUCATION","EMPL_TYPE","EMPL_SIZE"]
for cat in cat_candidates:
    if cat in df.columns:
        grp = df.groupby(cat)[primary_target].mean().sort_values(ascending=False)
        display(grp.head(20))

"""## Part 3 — Preprocessing & Baseline model"""

work = df[df[primary_target].notna()].copy()

y = work[primary_target].astype(int)

id_like = {"ID","Номер варианта", primary_target}
feature_cols = [c for c in work.columns if c not in id_like]
num_feats = [c for c in feature_cols if pd.api.types.is_numeric_dtype(work[c])]
cat_feats = [c for c in feature_cols if c not in num_feats]

print(f"Using {len(num_feats)} numeric and {len(cat_feats)} categorical features.")

num_transformer = Pipeline(steps=[
    ("imp", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False))
])

cat_transformer = Pipeline(steps=[
    ("imp", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", num_transformer, num_feats),
        ("cat", cat_transformer, cat_feats),
    ]
)

clf = Pipeline(steps=[
    ("prep", preprocess),
    ("mdl", LogisticRegression(max_iter=1000))
])

strat = y if y.nunique() == 2 else None
X_train, X_test, y_train, y_test = train_test_split(work[feature_cols], y, test_size=0.25, random_state=42, stratify=strat)

clf.fit(X_train, y_train)

proba = clf.predict_proba(X_test)[:,1]
pred = (proba >= 0.5).astype(int)

auc = roc_auc_score(y_test, proba)
print("ROC AUC:", round(auc, 4))

print("\nClassification report:")
print(classification_report(y_test, pred))

fpr, tpr, thr = roc_curve(y_test, proba)
plt.figure()
plt.plot(fpr, tpr, label=f"AUC={auc:.3f}")
plt.plot([0,1],[0,1],"--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve — Logistic Regression")
plt.legend()
plt.tight_layout()
plt.savefig("roc_curve_logreg.png")
plt.show()

ConfusionMatrixDisplay.from_predictions(y_test, pred)
plt.title("Confusion Matrix — Logistic Regression")
plt.tight_layout()
plt.savefig("confusion_matrix_logreg.png")
plt.show()

ohe = clf.named_steps["prep"].named_transformers_["cat"].named_steps["onehot"]
num_names = num_feats
cat_names = list(ohe.get_feature_names_out(cat_feats)) if len(cat_feats)>0 else []
feat_names = np.array(num_names + cat_names)

coef = clf.named_steps["mdl"].coef_.ravel()
top_idx = np.argsort(np.abs(coef))[::-1][:30]
fi = pd.DataFrame({"feature": feat_names[top_idx], "coef": coef[top_idx]})
display(fi.head(30))